import pdfplumber
from pdf2image import convert_from_path
import pytesseract
import os
import re
from langdetect import detect, DetectorFactory
from collections import Counter

DetectorFactory.seed = 0  # ä¿æŒ langdetect ç»“æœç¨³å®š


def detect_language(text, min_chars=100):
    """æ”¹è¿›çš„è¯­è¨€æ£€æµ‹ï¼ˆå­—ç¬¦å æ¯” + langdetectï¼‰"""
    text = text or ""

    # ç»Ÿè®¡å­—ç¬¦å æ¯”
    chinese_count = len(re.findall(r"[\u4e00-\u9fff]", text))  # ä¸­æ–‡å­—ç¬¦
    japanese_count = len(re.findall(r"[\u3040-\u30ff]", text))  # æ—¥æ–‡å­—ç¬¦
    english_count = len(re.findall(r"[a-zA-Z]", text))  # è‹±æ–‡å­—æ¯
    total_chars = chinese_count + japanese_count + english_count

    # è®¡ç®—å æ¯”
    if total_chars == 0:
        return "en"

    chi_ratio = chinese_count / total_chars
    jpn_ratio = japanese_count / total_chars
    eng_ratio = english_count / total_chars

    # åˆ¤æ–­ä¸»è¦è¯­è¨€
    if eng_ratio > 0.8:
        return "en"
    elif chi_ratio > 0.5:
        return "zh-cn"
    elif jpn_ratio > 0.5:
        return "ja"

    if total_chars < min_chars:
        if chi_ratio >= max(eng_ratio, jpn_ratio):
            return "zh-cn"
        if jpn_ratio >= max(eng_ratio, chi_ratio):
            return "ja"
        return "en"

    # è¯­è¨€æ··åˆæ—¶æ‰ç”¨ langdetect è¿›ä¸€æ­¥æ£€æµ‹
    results = []
    for _ in range(5):  # è¿›è¡Œ5æ¬¡æ£€æµ‹ï¼Œæé«˜ç¨³å®šæ€§
        try:
            results.append(detect(text))
        except:
            continue

    if results:
        return Counter(results).most_common(1)[0][0]  # è¿”å›å‡ºç°æœ€å¤šæ¬¡çš„è¯­è¨€

    return "en"

def extract_text_from_pdf(pdf_path, output_folder):
    """è‡ªåŠ¨é€‰æ‹©åˆé€‚æ–¹æ³•æå– PDF æ–‡æœ¬ï¼Œå¹¶ä»ç¬¬5é¡µååˆ¤æ–­ä¸»è¦è¯­è¨€"""
    os.makedirs(output_folder, exist_ok=True)

    print(f"ğŸ“„ è§£æ PDF æ–‡ä»¶: {pdf_path}")

    is_text_pdf = False
    full_text = ""
    text_for_language_detection = ""  # ç”¨äºè¯­è¨€æ£€æµ‹çš„æ–‡æœ¬

    with pdfplumber.open(pdf_path) as pdf:
        if any(page.extract_text() for page in pdf.pages[:3]):
            is_text_pdf = True

        if is_text_pdf:
            print("ğŸ“„ è¯¥ PDF å…·æœ‰å¯é€‰æ–‡æœ¬ï¼Œä½¿ç”¨ pdfplumber æå–...")
            for i, page in enumerate(pdf.pages):
                text = page.extract_text() or ""
                full_text += text + "\n"

                # åªä»ç¬¬ 5 é¡µå¼€å§‹ç»Ÿè®¡è¯­è¨€
                if i >= 4:
                    text_for_language_detection += text + "\n"

                with open(f"{output_folder}/page_{i + 1}.txt", "w", encoding="utf-8") as f:
                    f.write(text)

            lang_sample = text_for_language_detection.strip() or full_text
            detected_lang = detect_language(lang_sample)
            # å­˜å…¥ç¯å¢ƒå˜é‡ï¼ˆé€‚ç”¨äºå½“å‰è¿è¡Œç¯å¢ƒï¼‰
            os.environ["DETECTED_LANG"] = detected_lang

            # å­˜å…¥æ–‡ä»¶ï¼Œä¾¿äºå…¶ä»– Python æ–‡ä»¶è®¿é—®
            lang_file = os.path.join(output_folder, "lang.txt")
            with open(lang_file, "w", encoding="utf-8") as f:
                f.write(detected_lang)
        else:
            print("ğŸ–¼ï¸ è¯¥ PDF ä¼¼ä¹æ˜¯å½±å°ç‰ˆï¼Œä½¿ç”¨ OCR è¯†åˆ«...")
            images = convert_from_path(pdf_path)
            for i, image in enumerate(images):
                text = pytesseract.image_to_string(image, lang="eng")  # å…ˆé»˜è®¤è‹±æ–‡
                full_text += text + "\n"

                # åªä»ç¬¬ 5 é¡µå¼€å§‹ç»Ÿè®¡è¯­è¨€
                if i >= 4:
                    text_for_language_detection += text + "\n"

                with open(f"{output_folder}/page_{i + 1}.txt", "w", encoding="utf-8") as f:
                    f.write(text)

            lang_sample = text_for_language_detection.strip() or full_text
            detected_lang = detect_language(lang_sample)

            # å­˜å…¥ç¯å¢ƒå˜é‡ï¼ˆé€‚ç”¨äºå½“å‰è¿è¡Œç¯å¢ƒï¼‰
            os.environ["DETECTED_LANG"] = detected_lang

            # å­˜å…¥æ–‡ä»¶ï¼Œä¾¿äºå…¶ä»– Python æ–‡ä»¶è®¿é—®
            lang_file = os.path.join(output_folder, "lang.txt")
            with open(lang_file, "w", encoding="utf-8") as f:
                f.write(detected_lang)

        print(f"ğŸŒ ä¸»è¦è¯­è¨€æ£€æµ‹ç»“æœï¼š{detected_lang}")

    return detected_lang

# import pdfplumber
# import os
# from langdetect import DetectorFactory
# from detect_language import detect_language
#
# DetectorFactory.seed = 0  # ä½¿ langdetect ç»“æœç¨³å®š
#
#
# def extract_text_from_pdf(pdf_path, output_folder):
#     """ä» PDF æå–æ–‡æœ¬ï¼Œå¹¶æ£€æµ‹ä¸»è¦è¯­è¨€"""
#     os.makedirs(output_folder, exist_ok=True)
#
#     print("ğŸ“„ è¯¥ PDF å…·æœ‰å¯é€‰æ–‡æœ¬ï¼Œä½¿ç”¨ pdfplumber æå–...")
#     full_text = ""
#
#     with pdfplumber.open(pdf_path) as pdf:
#         for i, page in enumerate(pdf.pages):
#             text = page.extract_text() or ""
#             full_text += text + "\n"
#             with open(f"{output_folder}/page_{i + 1}.txt", "w", encoding="utf-8") as f:
#                 f.write(text)
#
#     # è¯­è¨€æ£€æµ‹ï¼ˆä»ç¬¬5é¡µå¼€å§‹ï¼Œå¦‚æœé¡µæ•°ä¸è¶³5ï¼Œåˆ™æ£€æµ‹æ‰€æœ‰æ–‡æœ¬ï¼‰
#     if len(pdf.pages) >= 5:
#         lang_text = full_text[full_text.find(pdf.pages[4].extract_text()):]
#     else:
#         lang_text = full_text
#
#     detected_lang = detect_language(lang_text)
#
#     # å­˜å…¥ç¯å¢ƒå˜é‡ï¼ˆé€‚ç”¨äºå½“å‰è¿è¡Œç¯å¢ƒï¼‰
#     os.environ["DETECTED_LANG"] = detected_lang
#
#     # å­˜å…¥æ–‡ä»¶ï¼Œä¾¿äºå…¶ä»– Python æ–‡ä»¶è®¿é—®
#     lang_file = os.path.join(output_folder, "lang.txt")
#     with open(lang_file, "w", encoding="utf-8") as f:
#         f.write(detected_lang)
#
#     print(f"ğŸŒ ä¸»è¦è¯­è¨€æ£€æµ‹ç»“æœï¼š{detected_lang}")
#     print(f"âœ… æå–å®Œæˆï¼Œæ–‡æœ¬å·²ä¿å­˜è‡³ {output_folder}")
#
#
